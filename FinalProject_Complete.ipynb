{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy as sp \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm \n",
    "import statsmodels.formula.api as smf \n",
    "import pandas as pd \n",
    "from statsmodels.stats.outliers_influence import OLSInfluence \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor \n",
    "from scipy.stats.mstats import zscore \n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "**Job Proficiency**\n",
    "A personnel officer in a governmental agency administered four newly developed aptitude tests to each of 25 applicants for entry-level clerical positions in the agency. For purpose of the study, all 25 applicants were accepted for positions irrespective of their test scores. After a probationary period, each applicant was rated for proficiency on the job. The scores on the four tests $(X_1, X_2, X_3, X_4)$ (Columns 2-5) and the job proficiency score ($Y$) (Column 1) for the 25 employees are stored in \"JobProficiency.txt\".\n",
    "\n",
    "(A1) Fit the multiple regression function containing all four predictor variables as first-order\n",
    "terms. Does it appear that all predictor variables should be retained?\n",
    "\n",
    "(A2) Using forward stepwise regression, find the best subset of predictor variables to predict job\n",
    "proficiency. Use $\\alpha$ limits of $.05$ and $.10$ for adding or deleting a variable, respectively.\n",
    "\n",
    "(A3) To assess internally the predictive ability of\n",
    "the regression model identified in (A2). compute the PRESS statistic and compare it\n",
    "to SSE. What does this comparison suggest about the validity of MSE as an indicator of the\n",
    "predictive ability of the fitted model?\n",
    "\n",
    "(B1) To assess externally the validity of the\n",
    "regression model identified in Part (A2). 25 additional applicants for entry-level clerical\n",
    "positions in the agency were similarly tested and hired irrespective of their test scores. The data are stored in\n",
    "JobProficiency_validation.txt. Fit the regression model identified in (A2) to the validation data set. Compare the\n",
    "estimated regression coefficients and their estimated standard deviations to those obtained\n",
    "in (A2). Also compare the MSE and coefficients of multiple determination. Do the estimates for the validation data set appear to be reasonably similar to those obtained for the model-building data set?\n",
    "\n",
    "(B2) Use the model obtained from (A2) to calculate the mean squared prediction error (MSPR) for the validation data and compare it to MSE obtained from\n",
    "the model-building data set. Is there evidence of a substantial bias problem in MSE here?\n",
    "\n",
    "(C1) For this question and the following ones, consider that a subset model containing only first-order\n",
    "terms in $x_1$ and $x_3$ is to be evaluated in detail, and the training data are used. Obtain the studentized deleted residuals and identify any outlying $y$ observations. Use $\\alpha=0.05$.\n",
    "\n",
    "(C2) Obtain the diagonal elements of the hat matrix. Using the rule of thumb discussed in the notes, identify\n",
    "any outlying $x$ observations.\n",
    "\n",
    "(C3) For those outlying cases found in (C1) and (C2), obtain DFFITS, DFBETAS\n",
    "and Cook's distance values for these cases to assess their influence. What do you conclude?\n",
    "\n",
    "(C4) Obtain the variance inflation factors. What do they indicate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JobProficiency</th>\n",
       "      <th>Test1</th>\n",
       "      <th>Test2</th>\n",
       "      <th>Test3</th>\n",
       "      <th>Test4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>80.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>96.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>76.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>73.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>58.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>116.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>104.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>99.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>64.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>126.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>94.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>71.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>111.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>109.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>127.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>82.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>67.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>109.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>78.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>115.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>83.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    JobProficiency  Test1  Test2  Test3  Test4\n",
       "0             88.0   86.0  110.0  100.0   87.0\n",
       "1             80.0   62.0   97.0   99.0  100.0\n",
       "2             96.0  110.0  107.0  103.0  103.0\n",
       "3             76.0  101.0  117.0   93.0   95.0\n",
       "4             80.0  100.0  101.0   95.0   88.0\n",
       "5             73.0   78.0   85.0   95.0   84.0\n",
       "6             58.0  120.0   77.0   80.0   74.0\n",
       "7            116.0  105.0  122.0  116.0  102.0\n",
       "8            104.0  112.0  119.0  106.0  105.0\n",
       "9             99.0  120.0   89.0  105.0   97.0\n",
       "10            64.0   87.0   81.0   90.0   88.0\n",
       "11           126.0  133.0  120.0  113.0  108.0\n",
       "12            94.0  140.0  121.0   96.0   89.0\n",
       "13            71.0   84.0  113.0   98.0   78.0\n",
       "14           111.0  106.0  102.0  109.0  109.0\n",
       "15           109.0  109.0  129.0  102.0  108.0\n",
       "16           100.0  104.0   83.0  100.0  102.0\n",
       "17           127.0  150.0  118.0  107.0  110.0\n",
       "18            99.0   98.0  125.0  108.0   95.0\n",
       "19            82.0  120.0   94.0   95.0   90.0\n",
       "20            67.0   74.0  121.0   91.0   85.0\n",
       "21           109.0   96.0  114.0  114.0  103.0\n",
       "22            78.0  104.0   73.0   93.0   80.0\n",
       "23           115.0   94.0  121.0  115.0  104.0\n",
       "24            83.0   91.0  129.0   97.0   83.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the data \n",
    "df = pd.read_csv('JobProficiency.txt', delim_whitespace=True, \n",
    "                 names = ['JobProficiency', 'Test1', 'Test2', 'Test3', 'Test4']) \n",
    "n = df.shape[0] \n",
    "df.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will fit the multiple Regression function containing all four predictor variables as first-order terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:         JobProficiency   R-squared:                       0.963\n",
      "Model:                            OLS   Adj. R-squared:                  0.955\n",
      "Method:                 Least Squares   F-statistic:                     129.7\n",
      "Date:                Tue, 14 Jul 2020   Prob (F-statistic):           5.26e-14\n",
      "Time:                        14:24:29   Log-Likelihood:                -67.951\n",
      "No. Observations:                  25   AIC:                             145.9\n",
      "Df Residuals:                      20   BIC:                             152.0\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   -124.3818      9.941    -12.512      0.000    -145.119    -103.645\n",
      "Test1          0.2957      0.044      6.725      0.000       0.204       0.387\n",
      "Test2          0.0483      0.057      0.853      0.404      -0.070       0.166\n",
      "Test3          1.3060      0.164      7.959      0.000       0.964       1.648\n",
      "Test4          0.5198      0.132      3.940      0.001       0.245       0.795\n",
      "==============================================================================\n",
      "Omnibus:                        3.256   Durbin-Watson:                   1.148\n",
      "Prob(Omnibus):                  0.196   Jarque-Bera (JB):                1.419\n",
      "Skew:                           0.139   Prob(JB):                        0.492\n",
      "Kurtosis:                       1.867   Cond. No.                     2.47e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.47e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "lm = smf.ols('JobProficiency~Test1+Test2+Test3+Test4', data=df).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:         JobProficiency   R-squared:                       0.962\n",
      "Model:                            OLS   Adj. R-squared:                  0.956\n",
      "Method:                 Least Squares   F-statistic:                     175.0\n",
      "Date:                Tue, 14 Jul 2020   Prob (F-statistic):           5.16e-15\n",
      "Time:                        14:24:29   Log-Likelihood:                -68.397\n",
      "No. Observations:                  25   AIC:                             144.8\n",
      "Df Residuals:                      21   BIC:                             149.7\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   -124.2000      9.874    -12.578      0.000    -144.734    -103.666\n",
      "Test1          0.2963      0.044      6.784      0.000       0.205       0.387\n",
      "Test3          1.3570      0.152      8.937      0.000       1.041       1.673\n",
      "Test4          0.5174      0.131      3.948      0.001       0.245       0.790\n",
      "==============================================================================\n",
      "Omnibus:                        2.687   Durbin-Watson:                   1.203\n",
      "Prob(Omnibus):                  0.261   Jarque-Bera (JB):                1.314\n",
      "Skew:                           0.154   Prob(JB):                        0.519\n",
      "Kurtosis:                       1.920   Cond. No.                     2.10e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.1e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "lm = smf.ols('JobProficiency~Test1+Test3+Test4', data=df).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the fitted Regression function is \n",
    "\n",
    "$$\\hat{y} = -124.200 +  0.29634x_1 + 1.3570x_3 + 0.51744x_4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiple Regression Function containing all four predictor variables is the following:\n",
    "\n",
    "$$\\hat{y} = -124.382 + 0.2957x_1 + 0.0483x_2 + 1.3060x_3 + 0.5198x_ 4$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regress absolute residual against Job Proficiency:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 AbsRes   R-squared:                       0.047\n",
      "Model:                            OLS   Adj. R-squared:                 -0.089\n",
      "Method:                 Least Squares   F-statistic:                    0.3472\n",
      "Date:                Tue, 14 Jul 2020   Prob (F-statistic):              0.792\n",
      "Time:                        14:24:29   Log-Likelihood:                -53.224\n",
      "No. Observations:                  25   AIC:                             114.4\n",
      "Df Residuals:                      21   BIC:                             119.3\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      4.4073      5.382      0.819      0.422      -6.784      15.599\n",
      "Test1          0.0020      0.024      0.083      0.935      -0.048       0.051\n",
      "Test3         -0.0760      0.083     -0.919      0.369      -0.248       0.096\n",
      "Test4          0.0649      0.071      0.909      0.374      -0.084       0.213\n",
      "==============================================================================\n",
      "Omnibus:                        4.326   Durbin-Watson:                   2.192\n",
      "Prob(Omnibus):                  0.115   Jarque-Bera (JB):                1.604\n",
      "Skew:                          -0.134   Prob(JB):                        0.448\n",
      "Kurtosis:                       1.788   Cond. No.                     2.10e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.1e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Regress absolute residual against JobProficiency\n",
    "df['AbsRes'] = np.abs(lm.resid) \n",
    "lm1 = smf.ols('AbsRes~Test1+Test3+Test4', data=df).fit()\n",
    "print('Regress absolute residual against Job Proficiency:') \n",
    "print(lm1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the estimated Standard Deviation function is \n",
    "\n",
    "$$\\hat{s} = 4.6072 + 0.045x_1 + -0.0682x_3 + 0.0511x_4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Futhermore, we will obtain the ANOVA table decomposition to help me the decision on if all four predictor variables should be retained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            df       sum_sq      mean_sq           F        PR(>F)\n",
      "Test1      1.0  2395.854657  2395.854657  142.619937  1.480226e-10\n",
      "Test2      1.0  1806.965411  1806.965411  107.564661  1.707742e-09\n",
      "Test3      1.0  4254.459243  4254.459243  253.258564  8.044871e-13\n",
      "Test4      1.0   260.743166   260.743166   15.521465  8.099714e-04\n",
      "Residual  20.0   335.977523    16.798876         NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "# perform ordinary least squares using smf.ols INSTEAD of sm.OLS\n",
    "lm = smf.ols('JobProficiency~Test1+Test2+Test3+Test4', data=df).fit()\n",
    "# sm.stats.anova_lm generates the anova table \n",
    "table = sm.stats.anova_lm(lm) \n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Error Sum of Squares is the smallest when all four predictor variables ($x_1, x_2, x_3, x_4$) are in the Regression Model;\n",
    "\n",
    "$$SSR(x_1, x_2, x_3, x_4) = 260.7431$$\n",
    "\n",
    "which is noticabily smaller than \n",
    "\n",
    "$SSR(x_1) = 2, 395.8547$\n",
    "\n",
    "$SSR(x_1, x_2) = 1, 806.9654$\n",
    "\n",
    "$SSR(x_1, x_2, x_3) = 4, 254.4592$\n",
    "\n",
    "Thus, it appers that all predictor variables should be retained. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A2) \n",
    "\n",
    "Using Forward Stepwise Regression, find the best subset of predictor variables to predict Job Proficiency. Use $\\alpha$ limits of 0.05 and 0.10 for adding or deleting a variable, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check what X looks like : \n",
      "     Test1  Test2  Test3  Test4    AbsRes\n",
      "0    86.0  110.0  100.0   87.0  6.003588\n",
      "1    62.0   97.0   99.0  100.0  0.254096\n",
      "2   110.0  107.0  103.0  103.0  5.457877\n",
      "3   101.0  117.0   93.0   95.0  5.081899\n",
      "4   100.0  101.0   95.0   88.0  0.122440\n",
      "5    78.0   85.0   95.0   84.0  1.711297\n",
      "6   120.0   77.0   80.0   74.0  0.205670\n",
      "7   105.0  122.0  116.0  102.0  1.099404\n",
      "8   112.0  119.0  106.0  105.0  3.156274\n",
      "9   120.0   89.0  105.0   97.0  5.030545\n",
      "10   87.0   81.0   90.0   88.0  5.240484\n",
      "11  133.0  120.0  113.0  108.0  1.569844\n",
      "12  140.0  121.0   96.0   89.0  0.395012\n",
      "13   84.0  113.0   98.0   78.0  3.033035\n",
      "14  106.0  102.0  109.0  109.0  0.518905\n",
      "15  109.0  129.0  102.0  108.0  6.608311\n",
      "16  104.0   83.0  100.0  102.0  4.908403\n",
      "17  150.0  118.0  107.0  110.0  4.639265\n",
      "18   98.0  125.0  108.0   95.0  1.547434\n",
      "19  120.0   94.0   95.0   90.0  4.838922\n",
      "20   74.0  121.0   91.0   85.0  1.807050\n",
      "21   96.0  114.0  114.0  103.0  3.235956\n",
      "22  104.0   73.0   93.0   80.0  3.790441\n",
      "23   94.0  121.0  115.0  104.0  1.482307\n",
      "24   91.0  129.0   97.0   83.0  5.662545\n",
      "Check what y looks like : \n",
      " 0    88.0\n",
      "1    80.0\n",
      "2    96.0\n",
      "3    76.0\n",
      "4    80.0\n",
      "Name: JobProficiency, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# y vector: JobProficiency \n",
    "y = df['JobProficiency'] \n",
    "# X matrix that contains the values of the predictor variables \n",
    "X = df.drop(columns = ['JobProficiency'], axis = 1) \n",
    "print('Check what X looks like :', '\\n', X.head(n))\n",
    "print('Check what y looks like :', '\\n', y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will perform the Stepwise Forward Regression on the Job Proficiency Data. The maximum acceptable $\\alpha$ limit for adding a variable is 0.05 and the minimum acceptable limit for removing $\\alpha$ variable is 0.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the code to perform stepwise forward selection \n",
    "def stepwise_selection(X, y,\n",
    "                       initial_list=[],\n",
    "                       threshold_in=0.05,\n",
    "                       threshold_out = 0.10,\n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection\n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments: \n",
    "        X - pandas.DataFrame with candidate features \n",
    "        y - list-like with the target \n",
    "        initial_list - list of features to start with (column names of X) \n",
    "        threshold_in - include a feature if its p-value < threshold_in \n",
    "        threshold_out - exclude a feature if its p-value > threshold_out \n",
    "        verbose - whether to print the sequence of inclusions and exclusions \n",
    "    Returns: list of selected features Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    \"\"\" \n",
    "    included = list(initial_list) \n",
    "    while True: \n",
    "        changed = False \n",
    "        # forward step \n",
    "        excluded = list(set(X.columns)-set(included)) \n",
    "        new_pval = pd.Series(index=excluded) \n",
    "        for new_column in excluded:\n",
    "            X1 = sm.add_constant(pd.DataFrame(X[included+[new_column]])) \n",
    "            results = sm.OLS(y, X1).fit() \n",
    "            new_pval[new_column] = results.pvalues[new_column] \n",
    "        best_pval = new_pval.min() \n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.argmin() \n",
    "            included.append(best_feature) \n",
    "            changed=True \n",
    "            if verbose: \n",
    "                print('Add {:30} with p-value {:.6}'.format(best_feature,\n",
    "best_pval))\n",
    "                \n",
    "        # backward step \n",
    "        X1 = sm.add_constant(pd.DataFrame(X[included])) \n",
    "        results = sm.OLS(y, X1).fit() \n",
    "        # use all coefs except intercept\n",
    "        pvalues = results.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty \n",
    "        if worst_pval > threshold_out: \n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax() \n",
    "            included.remove(worst_feature) \n",
    "            if verbose: \n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature,\n",
    "worst_pval)) \n",
    "        if not changed:\n",
    "            break\n",
    "    return included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add Test3                          with p-value 1.26431e-09\n",
      "Add Test1                          with p-value 1.57777e-06\n",
      "Add Test4                          with p-value 0.000735361\n",
      "The selected predictors by stepwise forward selection are:\n",
      "['Test3', 'Test1', 'Test4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12563\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\Users\\12563\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: FutureWarning: \n",
      "The current behaviour of 'Series.argmin' is deprecated, use 'idxmin'\n",
      "instead.\n",
      "The behavior of 'argmin' will be corrected to return the positional\n",
      "minimum in the future. For now, use 'series.values.argmin' or\n",
      "'np.argmin(np.array(values))' to get the position of the minimum\n",
      "row.\n"
     ]
    }
   ],
   "source": [
    "predictors = stepwise_selection(X, y) \n",
    "print('The selected predictors by stepwise forward selection are:')\n",
    "print(predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the best subset of predictor variables to predict $y$, job proficiency using $\\alpha$ limits of 0.05 and 0.10 respectively is the following: \n",
    "\n",
    "$$(x_1, x_3, x_4)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:         JobProficiency   R-squared (uncentered):                   0.987\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.984\n",
      "Method:                 Least Squares   F-statistic:                              300.6\n",
      "Date:                Tue, 14 Jul 2020   Prob (F-statistic):                    4.24e-18\n",
      "Time:                        14:24:34   Log-Likelihood:                         -94.933\n",
      "No. Observations:                  25   AIC:                                      199.9\n",
      "Df Residuals:                      20   BIC:                                      206.0\n",
      "Df Model:                           5                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Test1          0.1505      0.125      1.206      0.242      -0.110       0.411\n",
      "Test2          0.0404      0.167      0.242      0.811      -0.308       0.389\n",
      "Test3          0.0843      0.397      0.212      0.834      -0.744       0.912\n",
      "Test4          0.7068      0.393      1.798      0.087      -0.113       1.527\n",
      "AbsRes        -0.7292      1.170     -0.623      0.540      -3.170       1.712\n",
      "==============================================================================\n",
      "Omnibus:                        1.231   Durbin-Watson:                   2.134\n",
      "Prob(Omnibus):                  0.540   Jarque-Bera (JB):                0.923\n",
      "Skew:                          -0.159   Prob(JB):                        0.630\n",
      "Kurtosis:                       2.114   Cond. No.                         99.3\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "results = sm.OLS(y, X).fit() \n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 =  0.9868697666466971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find R^2 \n",
    "print('R2 = ', results.rsquared) \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (A3)\n",
    "\n",
    "To assess internally the predictive ability of the Regression Model identified in (A2) compute the PRESS statistic and compare it to SSE. \n",
    "\n",
    "What does this comparison suggest about the validity of MSE as an indicator of the predicitive ability of the fitted model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PRESS prediction error for the ith case is :\n",
    "\n",
    "$$y_i - \\hat{y}_{i(i)}$$\n",
    "\n",
    "and the PRESS_p criterion is the sum of the squares prediction errors over all $n$ cases:\n",
    "\n",
    "$$PRESS_P = \\sum (y_i - \\hat{y}_{i(i)}) ^2$$\n",
    "\n",
    "Also, note that \n",
    "\n",
    "$$SSE = \\sum (y_i - \\hat{y}_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.T times X =  \n",
      " [[276970.         276626.         261247.         246352.\n",
      "    8070.70018978]\n",
      " [276626.         291906.         270842.         254364.\n",
      "    8280.7131696 ]\n",
      " [261247.         270842.         255898.         240368.\n",
      "    7775.70257419]\n",
      " [246352.         254364.         240368.         226843.\n",
      "    7374.38886262]\n",
      " [  8070.70018978   8280.7131696    7775.70257419   7374.38886262\n",
      "     348.19700766]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct X^TX:\n",
    "X_t_X = np.dot(X.T, X) \n",
    "print('X.T times X = ', '\\n', X_t_X) \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.T times y =  \n",
      " [243112.         249996.         236047.         222564.\n",
      "   7162.76919187]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct X^Ty: \n",
    "X_t_y = np.dot(X.T, y)\n",
    "print('X.T times y = ', '\\n', X_t_y)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b =  \n",
      " [ 0.15048568  0.04039749  0.08427876  0.7068133  -0.72923747]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute b: \n",
    "b = np.dot(np.linalg.inv(X_t_X), X_t_y)\n",
    "print('b = ', '\\n', b) \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat =  \n",
      " [ 82.92808392  92.08829906  98.37834948  91.20483682  89.24547859\n",
      "  81.30252699  80.06539061 101.79905574 102.50896876  95.39535571\n",
      "  82.32755191 109.576841    96.66517823  78.38466606 105.92265497\n",
      " 101.7274569   95.94694351 110.72391982  94.91820521  89.94660146\n",
      "  82.45476535  99.10170661  80.21838459 101.15343631  81.61667626]\n",
      "\n",
      "e =  \n",
      " 0      5.071916\n",
      "1    -12.088299\n",
      "2     -2.378349\n",
      "3    -15.204837\n",
      "4     -9.245479\n",
      "5     -8.302527\n",
      "6    -22.065391\n",
      "7     14.200944\n",
      "8      1.491031\n",
      "9      3.604644\n",
      "10   -18.327552\n",
      "11    16.423159\n",
      "12    -2.665178\n",
      "13    -7.384666\n",
      "14     5.077345\n",
      "15     7.272543\n",
      "16     4.053056\n",
      "17    16.276080\n",
      "18     4.081795\n",
      "19    -7.946601\n",
      "20   -15.454765\n",
      "21     9.898293\n",
      "22    -2.218385\n",
      "23    13.846564\n",
      "24     1.383324\n",
      "Name: JobProficiency, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fitted values: \n",
    "y_hat = np.dot(X, b) \n",
    "print('y_hat = ', '\\n', y_hat) \n",
    "print()\n",
    "# Residual:\n",
    "e = y-y_hat\n",
    "print('e = ', '\\n', e) \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y^Ty =  221575.0\n",
      "\n",
      "1/ny^TJy =  212521.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute y^Ty \n",
    "y_t_y = np.dot(y, y)\n",
    "print('y^Ty = ', y_t_y) \n",
    "print()\n",
    "\n",
    "# Compute (1/n)y^TJy\n",
    "J = np.ones((n,n)) \n",
    "y_t_j_y = (1/n)*np.dot(y, np.dot(J, y)) \n",
    "print('1/ny^TJy = ', y_t_j_y)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y^Ty =  221575.0\n",
      "\n",
      "SSE =  2909.331455256208\n",
      "\n",
      "SSTO =  9054.0\n",
      "\n",
      "SSR =  6144.668544743792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute y^Ty \n",
    "y_t_y = np.dot(y, y) \n",
    "print('y^Ty = ', y_t_y) \n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "# SSE \n",
    "SSE = y_t_y - np.dot(b, X_t_y) \n",
    "print('SSE = ', SSE) \n",
    "print()\n",
    "\n",
    "# SSTO \n",
    "SSTO = y_t_y - y_t_j_y \n",
    "print('SSTO = ', SSTO) \n",
    "print()\n",
    "\n",
    "# SSR \n",
    "SSR = SSTO - SSE \n",
    "print('SSR = ', SSR) \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, \n",
    "\n",
    "$$SSE = 2, 965.8118$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will obtain $MSE$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 138.53959310743846\n"
     ]
    }
   ],
   "source": [
    "MSE = SSE / 21\n",
    "print('MSE =', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     6.470945\n",
      "1     0.132517\n",
      "2    -5.841694\n",
      "3    -7.451249\n",
      "4     0.124475\n",
      "5     2.979700\n",
      "6     0.361166\n",
      "7    -1.365112\n",
      "8    -3.859519\n",
      "9    -4.808616\n",
      "10   -5.536303\n",
      "11    1.879782\n",
      "12   -0.720008\n",
      "13   -4.584906\n",
      "14    0.111044\n",
      "15    7.662001\n",
      "16    7.524138\n",
      "17    6.237989\n",
      "18   -2.375423\n",
      "19   -5.035669\n",
      "20    0.862844\n",
      "21   -3.521341\n",
      "22    6.897026\n",
      "23    1.810849\n",
      "24    5.795147\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "t = OLSInfluence(lm).resid_press\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5295752.55538795\n"
     ]
    }
   ],
   "source": [
    "press_statistic = np.sum(y-t)\n",
    "press_statistic_2 = np.square(press_statistic)\n",
    "\n",
    "print(press_statistic_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the PRESS Statistic $PRESS_p = 5,295,752.55$. \n",
    "\n",
    "Recall that $SSE = 2, 965.8118$.\n",
    "\n",
    "Comparing the two we can see that the Press Statistic is much larger than $SSE$. \n",
    "\n",
    "Models with small $PRESS_p$ values are considered good candidate models. Also, having a large SSE is not favorable. Thus, this model may need to be reconsidered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B1)\n",
    "\n",
    "To assess externally the validity of the Regression Model identified in Part (A2) 25 additional applicants for entry-level clerical positions in the agency were similarly tested and hired irrespective of their test scores. the data is stored in JobProficiency_validation.txt. \n",
    "\n",
    "We will fit the Regression Model identified in (A2) to the vaildation data set. \n",
    "\n",
    "We will then compare the estimared Regression Coefficients and their estimated standard deviations to those obtained in (A2). \n",
    "\n",
    "Also, compare the MSE and coefficients of multiple determination. \n",
    "\n",
    "Do the estimates for the validation data set appear to be reasonibly similar to those obtained for the model-building data set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JobProficiency</th>\n",
       "      <th>Test1</th>\n",
       "      <th>Test2</th>\n",
       "      <th>Test3</th>\n",
       "      <th>Test4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>92.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>71.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>77.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>92.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>66.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>61.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>57.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>66.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>75.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>98.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>100.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>67.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>111.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>97.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>74.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>117.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>92.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>95.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>104.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>100.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>95.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>81.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>109.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    JobProficiency  Test1  Test2  Test3  Test4\n",
       "0             58.0   65.0  109.0   88.0   84.0\n",
       "1             92.0   85.0   90.0  104.0   98.0\n",
       "2             71.0   93.0   73.0   91.0   82.0\n",
       "3             77.0   95.0   57.0   95.0   85.0\n",
       "4             92.0  102.0  139.0  101.0   92.0\n",
       "5             66.0   63.0  101.0   93.0   84.0\n",
       "6             61.0   81.0  129.0   88.0   76.0\n",
       "7             57.0  111.0  102.0   83.0   72.0\n",
       "8             66.0   67.0   98.0   98.0   84.0\n",
       "9             75.0   91.0  111.0   96.0   84.0\n",
       "10            98.0  128.0   99.0   98.0   89.0\n",
       "11           100.0  116.0  103.0  103.0  103.0\n",
       "12            67.0  105.0  102.0   88.0   83.0\n",
       "13           111.0   99.0  132.0  109.0  105.0\n",
       "14            97.0   93.0   95.0  106.0   98.0\n",
       "15            99.0   99.0  113.0  104.0   95.0\n",
       "16            74.0  110.0  114.0   91.0   78.0\n",
       "17           117.0  128.0  134.0  108.0   98.0\n",
       "18            92.0   99.0  110.0   96.0   97.0\n",
       "19            95.0  111.0  113.0  101.0   91.0\n",
       "20           104.0  109.0  120.0  104.0  106.0\n",
       "21           100.0   78.0  125.0  115.0  102.0\n",
       "22            95.0  115.0  119.0  102.0   94.0\n",
       "23            81.0  129.0   70.0   94.0   95.0\n",
       "24           109.0  136.0  104.0  106.0  104.0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the data \n",
    "df_v = pd.read_csv('JobProficiency_validation.txt', delim_whitespace=True, \n",
    "                 names = ['JobProficiency', 'Test1', 'Test2', 'Test3', 'Test4']) \n",
    "n = df_v.shape[0] \n",
    "df_v.head(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, We will fit the Regression Model identified in (A2) to the vaildation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:         JobProficiency   R-squared:                       0.949\n",
      "Model:                            OLS   Adj. R-squared:                  0.942\n",
      "Method:                 Least Squares   F-statistic:                     130.0\n",
      "Date:                Fri, 03 Jul 2020   Prob (F-statistic):           1.02e-13\n",
      "Time:                        13:10:52   Log-Likelihood:                -69.668\n",
      "No. Observations:                  25   AIC:                             147.3\n",
      "Df Residuals:                      21   BIC:                             152.2\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   -122.7671     11.848    -10.362      0.000    -147.406     -98.128\n",
      "Test1          0.3124      0.047      6.605      0.000       0.214       0.411\n",
      "Test3          1.4068      0.233      6.048      0.000       0.923       1.891\n",
      "Test4          0.4284      0.197      2.169      0.042       0.018       0.839\n",
      "==============================================================================\n",
      "Omnibus:                        1.376   Durbin-Watson:                   1.251\n",
      "Prob(Omnibus):                  0.503   Jarque-Bera (JB):                0.842\n",
      "Skew:                          -0.449   Prob(JB):                        0.657\n",
      "Kurtosis:                       2.944   Cond. No.                     2.33e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.33e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "lm = smf.ols('JobProficiency~Test1+Test3+Test4', data=df_v).fit()\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the fitted Regression function is \n",
    "\n",
    "$$\\hat{y} = -122.7671 +  0.3124x_1 + 1.4068x_3 + 0.4284x_4$$\n",
    "\n",
    "For (A2):  the fitted Regression function is \n",
    "\n",
    "$$\\hat{y} = -124.200 +  0.29634x_1 + 1.3570x_3 + 0.51744x_4$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regress absolute residual against Job Proficiency:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 AbsRes   R-squared:                       0.117\n",
      "Model:                            OLS   Adj. R-squared:                 -0.009\n",
      "Method:                 Least Squares   F-statistic:                    0.9254\n",
      "Date:                Fri, 03 Jul 2020   Prob (F-statistic):              0.446\n",
      "Time:                        13:10:53   Log-Likelihood:                -56.310\n",
      "No. Observations:                  25   AIC:                             120.6\n",
      "Df Residuals:                      21   BIC:                             125.5\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -5.1862      6.944     -0.747      0.463     -19.626       9.254\n",
      "Test1          0.0235      0.028      0.847      0.407      -0.034       0.081\n",
      "Test3          0.0109      0.136      0.080      0.937      -0.273       0.294\n",
      "Test4          0.0530      0.116      0.458      0.652      -0.188       0.294\n",
      "==============================================================================\n",
      "Omnibus:                        3.385   Durbin-Watson:                   2.340\n",
      "Prob(Omnibus):                  0.184   Jarque-Bera (JB):                2.565\n",
      "Skew:                           0.783   Prob(JB):                        0.277\n",
      "Kurtosis:                       2.892   Cond. No.                     2.33e+03\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.33e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Regress absolute residual against JobProficiency\n",
    "df_v['AbsRes'] = np.abs(lm.resid) \n",
    "lm1 = smf.ols('AbsRes~Test1+Test3+Test4', data=df_v).fit()\n",
    "print('Regress absolute residual against Job Proficiency:') \n",
    "print(lm1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the estimated Standard Deviation function is \n",
    "\n",
    "$$\\hat{s} = -5.1862 + 0.0235x_1 + 0.0109x_3 + 0.0530x_4$$\n",
    "\n",
    "(A2) estimated Standard Deviation function is \n",
    "\n",
    "$$\\hat{s} = 4.6072 + 0.045x_1 + -0.0682x_3 + 0.0511x_4$$\n",
    "\n",
    "The estimated Regression Coeffiencets are very similar in both cases.\n",
    "\n",
    "However, the estimated Standard Deviations of the estimated Regression Coeffiecients differ pretty greatly in the cases with the intercept  and the $x_3$ term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will obtain MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 132.24233887528217\n"
     ]
    }
   ],
   "source": [
    "MSE = SSE / 22\n",
    "\n",
    "print('MSE =', MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the two $MSE$ values we see that they are the same. \n",
    "\n",
    "Now, we will obtain the coefficient of Multiple Determination for the Validation Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:         JobProficiency   R-squared (uncentered):                   0.987\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.984\n",
      "Method:                 Least Squares   F-statistic:                              300.6\n",
      "Date:                Fri, 03 Jul 2020   Prob (F-statistic):                    4.24e-18\n",
      "Time:                        13:10:54   Log-Likelihood:                         -94.933\n",
      "No. Observations:                  25   AIC:                                      199.9\n",
      "Df Residuals:                      20   BIC:                                      206.0\n",
      "Df Model:                           5                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Test1          0.1505      0.125      1.206      0.242      -0.110       0.411\n",
      "Test2          0.0404      0.167      0.242      0.811      -0.308       0.389\n",
      "Test3          0.0843      0.397      0.212      0.834      -0.744       0.912\n",
      "Test4          0.7068      0.393      1.798      0.087      -0.113       1.527\n",
      "AbsRes        -0.7292      1.170     -0.623      0.540      -3.170       1.712\n",
      "==============================================================================\n",
      "Omnibus:                        1.231   Durbin-Watson:                   2.134\n",
      "Prob(Omnibus):                  0.540   Jarque-Bera (JB):                0.923\n",
      "Skew:                          -0.159   Prob(JB):                        0.630\n",
      "Kurtosis:                       2.114   Cond. No.                         99.3\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "results = sm.OLS(y, X).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 =  0.9868697666466971\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find R^2 \n",
    "print('R2 = ', results.rsquared) \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, for the Validation Data the coefficient of Multiple Determination is \n",
    "\n",
    "$$R^2 = 0.9866$$\n",
    "\n",
    "For the JobProficiency Data set the coefficient of Multiple Determination is \n",
    "\n",
    "$$R^2 = 0.9866$$\n",
    "\n",
    "Hence, they are the same in both cases. \n",
    "\n",
    "The estimates for the validation data set appear to be reasonably similar to those obtained from the model building data set most of the Regression values we compared between the two sets were the same or very close in value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (B2)\n",
    "\n",
    " Use the model obtained from (A2) to calculate the mean squared prediction error (MSRP) for the validation data and compare it to MSE obtained from the model-building data set. \n",
    "\n",
    "Is there evidence of a substantial bias problem in MSE here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will calculate the mean squared prediction error (MSRP) for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First read in the validation data\n",
    "df_val = pd.read_csv('JobProficiency_validation.txt', delim_whitespace=True, \n",
    "                     names = ['JobProficiency', 'Test1', 'Test2', 'Test3', 'Test4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Validation Data, MSPR =  13.958075258367568\n",
      "MSE =  16.798876132731827\n"
     ]
    }
   ],
   "source": [
    "lm = smf.ols('JobProficiency~Test1+Test2+Test3+Test4', data=df).fit() \n",
    "# Predict for validation data \n",
    "y_val = lm.predict(df_val) \n",
    "# MSPR\n",
    "MSPR = np.sum((df_val['JobProficiency'].to_numpy()-y_val)**2)/df_val.shape[0]\n",
    "print('For the Validation Data, MSPR = ', MSPR) \n",
    "# What about the MSE for the model based on the training data? \n",
    "MSE = lm.mse_resid \n",
    "print('MSE = ', MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the MSRP for the validation data set is \n",
    "\n",
    "$$MSRP = 13.4065$$\n",
    "\n",
    "Recall the MSE obtained from the model-building data set which is the following: \n",
    "\n",
    "$$MSE = 134.8096$$\n",
    "\n",
    "Comparing the MSRP for the Validation Data set and the MSE for the model-building data set we see that the MSE is much larger than MSRP.\n",
    "\n",
    "I believe that there is evidence of a substantial bias problem in MSE here as the closer to 0 MSE is the better & MSE is very large in thise case as well as MSE being much larger than the Mean Squared Prediction Error (MSRP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C1)\n",
    "\n",
    "For this question & the following ones, consider that a subset model containing only first-order terms in $x_1$ and $x_3$ is to be evaluated in detail, and the training data are used. Obtain the studentized deleted residuals and identify any outlying $y$ observations. \n",
    "\n",
    "Use $\\alpha = 0.05$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>JobProficiency</th>\n",
       "      <th>Test1</th>\n",
       "      <th>Test3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>88.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             JobProficiency  Test1  Test3\n",
       "88.0  86.0            110.0  100.0   87.0\n",
       "80.0  62.0             97.0   99.0  100.0\n",
       "96.0  110.0           107.0  103.0  103.0\n",
       "76.0  101.0           117.0   93.0   95.0\n",
       "80.0  100.0           101.0   95.0   88.0\n",
       "73.0  78.0             85.0   95.0   84.0\n",
       "58.0  120.0            77.0   80.0   74.0\n",
       "116.0 105.0           122.0  116.0  102.0\n",
       "104.0 112.0           119.0  106.0  105.0\n",
       "99.0  120.0            89.0  105.0   97.0\n",
       "64.0  87.0             81.0   90.0   88.0\n",
       "126.0 133.0           120.0  113.0  108.0\n",
       "94.0  140.0           121.0   96.0   89.0\n",
       "71.0  84.0            113.0   98.0   78.0\n",
       "111.0 106.0           102.0  109.0  109.0\n",
       "109.0 109.0           129.0  102.0  108.0\n",
       "100.0 104.0            83.0  100.0  102.0\n",
       "127.0 150.0           118.0  107.0  110.0\n",
       "99.0  98.0            125.0  108.0   95.0\n",
       "82.0  120.0            94.0   95.0   90.0\n",
       "67.0  74.0            121.0   91.0   85.0\n",
       "109.0 96.0            114.0  114.0  103.0\n",
       "78.0  104.0            73.0   93.0   80.0\n",
       "115.0 94.0            121.0  115.0  104.0\n",
       "83.0  91.0            129.0   97.0   83.0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the data \n",
    "df_1 = pd.read_csv('JobProficiency.txt', delim_whitespace=True, \n",
    "                 names = ['JobProficiency', 'Test1', 'Test3']) \n",
    "n = df_1.shape[0] \n",
    "df_1.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  i       ei        hii         ti    \n",
      "  1    3.307787   0.085485   0.649988 \n",
      "  2    5.493973   0.088065   1.100843 \n",
      "  3   -2.524810   0.079287  -0.492376 \n",
      "  4   -1.156637   0.127755  -0.230709 \n",
      "  5   -0.454591   0.059256  -0.087217 \n",
      "  6    0.211475   0.083438   0.041099 \n",
      "  7   -2.075658   0.270959  -0.454510 \n",
      "  8   -4.484195   0.209026  -0.958378 \n",
      "  9   -0.691342   0.081989  -0.134305 \n",
      " 10   -6.655796   0.051904  -1.323779 \n",
      " 11   -2.808615   0.113523  -0.559120 \n",
      " 12    1.228612   0.122195   0.244326 \n",
      " 13   -2.216097   0.053497  -0.425629 \n",
      " 14   -9.348887   0.229696  -2.198011 \n",
      " 15    2.929787   0.116587   0.584653 \n",
      " 16   12.646853   0.180590   3.156277 \n",
      " 17    9.035552   0.101700   1.923641 \n",
      " 18    7.244067   0.144911   1.537353 \n",
      " 19   -4.459348   0.106924  -0.894510 \n",
      " 20   -5.423741   0.058460  -1.067822 \n",
      " 21    2.898127   0.091213   0.569978 \n",
      " 22   -4.701666   0.148600  -0.968994 \n",
      " 23   -0.202010   0.122840  -0.040132 \n",
      " 24    0.172044   0.162718   0.034983 \n",
      " 25    2.035116   0.109382   0.402766 \n"
     ]
    }
   ],
   "source": [
    "# y against x1 and x2 \n",
    "lm = smf.ols('JobProficiency~Test1+Test3', data=df).fit() \n",
    "X = df_1[['Test1', 'Test3']].to_numpy() \n",
    "X = sm.add_constant(X) \n",
    "y = df['JobProficiency'].to_numpy() \n",
    "# Find the H matrix \n",
    "H = np.dot(np.dot(X, np.linalg.inv(np.dot(X.T,X))), X.T)\n",
    "# Find standardized delted residuals (externally studentized residuals) \n",
    "t = lm.resid * np.sqrt((n-3-1)/(lm.ssr*(1-np.diag(H))-lm.resid**2)) \n",
    "print('{:^5} {:^10} {:^10} {:^10}'.format('i', 'ei', 'hii', 'ti')) \n",
    "for i in range(1, n+1):\n",
    "    print('{:^5d} {:^10f} {:^10f} {:^10f}'.format(i, lm.resid[i-1], H[i-1,i-1],\n",
    "t[i-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t calculated from OLSInfluence is: \n",
      "88.0   86.0     0.253556\n",
      "80.0   62.0    -0.508568\n",
      "96.0   110.0   -0.112846\n",
      "76.0   101.0    1.302025\n",
      "80.0   100.0    0.007335\n",
      "73.0   78.0    -1.090111\n",
      "58.0   120.0   -0.649733\n",
      "116.0  105.0   -0.028171\n",
      "104.0  112.0    0.479370\n",
      "99.0   120.0   -1.508293\n",
      "64.0   87.0    -1.007591\n",
      "126.0  133.0    0.066469\n",
      "94.0   140.0    1.291054\n",
      "71.0   84.0     0.624421\n",
      "111.0  106.0   -0.876120\n",
      "109.0  109.0    1.593999\n",
      "100.0  104.0   -1.598369\n",
      "127.0  150.0    0.368011\n",
      "99.0   98.0     0.727923\n",
      "82.0   120.0   -0.446371\n",
      "67.0   74.0     1.715376\n",
      "109.0  96.0    -0.431160\n",
      "78.0   104.0   -1.910681\n",
      "115.0  94.0    -0.018627\n",
      "83.0   91.0     1.870476\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# y against x1 and x3\n",
    "lm = smf.ols('JobProficiency~Test1+Test3', data=df_1).fit() \n",
    "# Use OLSInfluence to find externally studentized residuals: \n",
    "t = OLSInfluence(lm).resid_studentized_external \n",
    "print('t calculated from OLSInfluence is: ') \n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that cases 19, 14, and 21 have the largest absoulte studentized Deleted Residuals. Incidentally, consideration of the residuals $e_i$ (Shown in Coulumn 1) here would have identified cases 19, 14, and 21 as the most outlying ones. We will test wheather case 19, which has the largest studentized Deleted Residual, is an outlier from a change in the model (using $\\alpha = 0.05$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t(1-alpha/2; n-p-1) =  2.079613844727662\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "print('t(1-alpha/2; n-p-1) = ', sp.stats.t.ppf(1-alpha/2, n-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since\n",
    "\n",
    "$$t \\big( \\frac{1-\\alpha}{2};n-p-1 \\big) = 2.0796$$\n",
    "\n",
    "and $|t_{19}|> t \\big( \\frac{1-\\alpha}{2};n-p-1 \\big)$, we will conclude that 19 is an outlier. \n",
    "\n",
    "Using the same argument, we can see that the rest of the points are not outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C2)\n",
    "\n",
    "Obtain the diagonal elements of the hat matrix. Using the rule of thumb discussed in the notes, identify any outlying $x$ observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted values are:  0      82.928084\n",
      "1      92.088299\n",
      "2      98.378349\n",
      "3      91.204837\n",
      "4      89.245479\n",
      "5      81.302527\n",
      "6      80.065391\n",
      "7     101.799056\n",
      "8     102.508969\n",
      "9      95.395356\n",
      "10     82.327552\n",
      "11    109.576841\n",
      "12     96.665178\n",
      "13     78.384666\n",
      "14    105.922655\n",
      "15    101.727457\n",
      "16     95.946944\n",
      "17    110.723920\n",
      "18     94.918205\n",
      "19     89.946601\n",
      "20     82.454765\n",
      "21     99.101707\n",
      "22     80.218385\n",
      "23    101.153436\n",
      "24     81.616676\n",
      "dtype: float64\n",
      "Residuals are:  0      5.071916\n",
      "1    -12.088299\n",
      "2     -2.378349\n",
      "3    -15.204837\n",
      "4     -9.245479\n",
      "5     -8.302527\n",
      "6    -22.065391\n",
      "7     14.200944\n",
      "8      1.491031\n",
      "9      3.604644\n",
      "10   -18.327552\n",
      "11    16.423159\n",
      "12    -2.665178\n",
      "13    -7.384666\n",
      "14     5.077345\n",
      "15     7.272543\n",
      "16     4.053056\n",
      "17    16.276080\n",
      "18     4.081795\n",
      "19    -7.946601\n",
      "20   -15.454765\n",
      "21     9.898293\n",
      "22    -2.218385\n",
      "23    13.846564\n",
      "24     1.383324\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Fitted values are: ', results.fittedvalues)\n",
    "print('Residuals are: ', results.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hat Matrix for the data is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H = \n",
      "[[ 8.54849007e-02 -4.97514826e-03 -1.64776731e-03 -6.70701228e-03\n",
      "   5.03632041e-02  7.64368634e-02  5.58111663e-02  7.92391801e-02\n",
      "   2.47737211e-03  4.89040343e-02  2.17599224e-02  2.29667221e-02\n",
      "   4.95654457e-02  1.32709321e-01 -6.43431811e-03 -3.99604978e-02\n",
      "  -1.22913215e-02 -2.43940456e-02  7.91028330e-02  3.73263745e-02\n",
      "   4.70358232e-02  6.12794526e-02  9.10692100e-02  6.04816941e-02\n",
      "   9.43965909e-02]\n",
      " [-4.97514826e-03  8.80651733e-02  7.91454415e-02  9.81425750e-02\n",
      "   3.75453780e-02  1.11278104e-02  5.28322946e-02 -2.08408998e-02\n",
      "   7.08045446e-02  2.51526363e-02  7.34615124e-02  4.03351322e-02\n",
      "   3.69665430e-02 -5.00482216e-02  7.56724316e-02  1.19350628e-01\n",
      "   9.40907302e-02  9.66432772e-02 -9.60582810e-03  5.07541618e-02\n",
      "   4.64651098e-02  1.29945812e-04 -9.23303405e-04 -4.48889167e-04\n",
      "  -9.84303522e-03]\n",
      " [-1.64776731e-03  7.91454415e-02  7.92872296e-02  7.59935354e-02\n",
      "   2.60976240e-02  2.58383325e-03  9.40018725e-03  1.65547284e-02\n",
      "   7.79239588e-02  3.52697660e-02  4.79645677e-02  6.49455808e-02\n",
      "   2.76026830e-02 -4.58070191e-02  8.83175834e-02  1.13052857e-01\n",
      "   8.65289481e-02  1.02942809e-01  1.03927044e-02  3.78545194e-02\n",
      "   2.59558359e-02  3.11799536e-02 -1.21831801e-02  3.26850126e-02\n",
      "  -1.20413919e-02]\n",
      " [-6.70701228e-03  9.81425750e-02  7.59935354e-02  1.27754777e-01\n",
      "   5.52871822e-02  2.63963234e-02  1.18483616e-01 -7.36350269e-02\n",
      "   5.75760769e-02  1.07486551e-02  1.10058662e-01  2.56414934e-03\n",
      "   5.15556010e-02 -4.98028528e-02  5.36040479e-02  1.23061405e-01\n",
      "   1.01633709e-01  8.27353546e-02 -3.65596622e-02  6.97326117e-02\n",
      "   7.74362219e-02 -4.45037202e-02  1.94140564e-02 -4.82353015e-02\n",
      "  -2.73498328e-03]\n",
      " [ 5.03632041e-02  3.75453780e-02  2.60976240e-02  5.52871822e-02\n",
      "   5.92562507e-02  6.38176367e-02  1.05321281e-01  1.15114866e-03\n",
      "   1.77968952e-02  2.89263459e-02  6.92896437e-02  3.29105373e-04\n",
      "   5.61092255e-02  6.46396800e-02  7.21547328e-03  2.24025700e-02\n",
      "   3.32580064e-02  1.00884840e-02  2.51870031e-02  5.69755576e-02\n",
      "   7.07040046e-02  4.02415936e-03  7.23923800e-02  8.77134234e-04\n",
      "   6.09446260e-02]\n",
      " [ 7.64368634e-02  1.11278104e-02  2.58383325e-03  2.63963234e-02\n",
      "   6.38176367e-02  8.34380141e-02  1.09346561e-01  2.75456717e-02\n",
      "  -2.59787604e-03  3.51000523e-02  5.61035044e-02 -6.51337382e-03\n",
      "   6.04553689e-02  1.17497060e-01 -1.75897740e-02 -2.34844650e-02\n",
      "   2.86044820e-03 -2.55805213e-02  4.95387204e-02  5.40074480e-02\n",
      "   7.23616139e-02  1.95549244e-02  9.99727386e-02  1.61926565e-02\n",
      "   9.14287614e-02]\n",
      " [ 5.58111663e-02  5.28322946e-02  9.40018725e-03  1.18483616e-01\n",
      "   1.05321281e-01  1.09346561e-01  2.70959063e-01 -1.20936223e-01\n",
      "  -2.29223133e-02 -4.76846721e-03  1.55837716e-01 -9.66642814e-02\n",
      "   9.42116742e-02  8.50746192e-02 -5.72574536e-02  1.44718747e-02\n",
      "   4.07163679e-02 -3.80571997e-02 -3.30656882e-02  1.03308641e-01\n",
      "   1.48753388e-01 -1.01735969e-01  1.33578414e-01 -1.12845576e-01\n",
      "   9.01463067e-02]\n",
      " [ 7.92391801e-02 -2.08408998e-02  1.65547284e-02 -7.36350269e-02\n",
      "   1.15114866e-03  2.75456717e-02 -1.20936223e-01  2.09025801e-01\n",
      "   4.62511073e-02  8.47422732e-02 -7.03382520e-02  1.26540376e-01\n",
      "   8.85039804e-03  1.10031097e-01  6.27502247e-02 -3.07363055e-02\n",
      "  -1.97402812e-02  2.75558337e-02  1.40833175e-01 -1.20461128e-02\n",
      "  -3.62444796e-02  1.73831410e-01  2.53444344e-02  1.81530659e-01\n",
      "   6.27400627e-02]\n",
      " [ 2.47737211e-03  7.08045446e-02  7.79239588e-02  5.75760769e-02\n",
      "   1.77968952e-02 -2.59787604e-03 -2.29223133e-02  4.62511073e-02\n",
      "   8.19888463e-02  4.32434699e-02  2.80177254e-02  8.29757623e-02\n",
      "   2.08514219e-02 -3.93225310e-02  9.62511193e-02  1.05461589e-01\n",
      "   7.89577642e-02  1.05438144e-01  2.69135861e-02  2.79942808e-02\n",
      "   1.06774810e-02  5.54381322e-02 -1.89043152e-02  5.84926590e-02\n",
      "  -1.17849010e-02]\n",
      " [ 4.89040343e-02  2.51526363e-02  3.52697660e-02  1.07486551e-02\n",
      "   2.89263459e-02  3.51000523e-02 -4.76846721e-03  8.47422732e-02\n",
      "   4.32434699e-02  5.19040302e-02  1.04920841e-02  6.44211566e-02\n",
      "   3.10697717e-02  5.54211689e-02  4.81303206e-02  2.38657807e-02\n",
      "   2.57526355e-02  3.92131893e-02  6.60514405e-02  2.58394928e-02\n",
      "   1.88092163e-02  7.58251419e-02  3.39000539e-02  7.79685677e-02\n",
      "   4.40171836e-02]\n",
      " [ 2.17599224e-02  7.34615124e-02  4.79645677e-02  1.10058662e-01\n",
      "   6.92896437e-02  5.61035044e-02  1.55837716e-01 -7.03382520e-02\n",
      "   2.80177254e-02  1.04920841e-02  1.13522830e-01 -2.40191312e-02\n",
      "   6.37395412e-02  9.78438357e-03  1.46639528e-02  7.32938791e-02\n",
      "   7.12079447e-02  3.56537623e-02 -2.26408974e-02  7.58827133e-02\n",
      "   9.47865884e-02 -4.93484426e-02  6.06106397e-02 -5.48985450e-02\n",
      "   3.51136950e-02]\n",
      " [ 2.29667221e-02  4.03351322e-02  6.49455808e-02  2.56414934e-03\n",
      "   3.29105373e-04 -6.51337382e-03 -9.66642814e-02  1.26540376e-01\n",
      "   8.29757623e-02  6.44211566e-02 -2.40191312e-02  1.22195153e-01\n",
      "   6.90937248e-03 -2.16815068e-03  1.04427183e-01  6.86290325e-02\n",
      "   4.86260191e-02  9.63985086e-02  7.56088589e-02  3.75034497e-03\n",
      "  -2.42813432e-02  1.18511701e-01 -2.30951476e-02  1.25091968e-01\n",
      "   1.51530100e-03]\n",
      " [ 4.95654457e-02  3.69665430e-02  2.76026830e-02  5.15556010e-02\n",
      "   5.61092255e-02  6.04553689e-02  9.42116742e-02  8.85039804e-03\n",
      "   2.08514219e-02  3.10697717e-02  6.37395412e-02  6.90937248e-03\n",
      "   5.34966266e-02  6.23963944e-02  1.19270892e-02  2.36960670e-02\n",
      "   3.32674082e-02  1.38926796e-02  2.86646539e-02  5.39361539e-02\n",
      "   6.54730856e-02  1.08159885e-02  6.78536384e-02  8.20338952e-03\n",
      "   5.84897784e-02]\n",
      " [ 1.32709321e-01 -5.00482216e-02 -4.58070191e-02 -4.98028528e-02\n",
      "   6.46396800e-02  1.17497060e-01  8.50746192e-02  1.10031097e-01\n",
      "  -3.93225310e-02  5.54211689e-02  9.78438357e-03 -2.16815068e-03\n",
      "   6.23963944e-02  2.29696307e-01 -5.92667327e-02 -1.22849803e-01\n",
      "  -6.55058521e-02 -9.44231962e-02  1.14763037e-01  3.82109902e-02\n",
      "   6.03984776e-02  7.48746331e-02  1.48412321e-01  7.26313475e-02\n",
      "   1.52653523e-01]\n",
      " [-6.43431811e-03  7.56724316e-02  8.83175834e-02  5.36040479e-02\n",
      "   7.21547328e-03 -1.75897740e-02 -5.72574536e-02  6.27502247e-02\n",
      "   9.62511193e-02  4.81303206e-02  1.46639528e-02  1.04427183e-01\n",
      "   1.19270892e-02 -5.92667327e-02  1.16587279e-01  1.20813838e-01\n",
      "   8.65853593e-02  1.25767983e-01  3.12586092e-02  1.96180969e-02\n",
      "  -5.42967855e-03  7.19309284e-02 -3.94156295e-02  7.66425443e-02\n",
      "  -2.67704777e-02]\n",
      " [-3.99604978e-02  1.19350628e-01  1.13052857e-01  1.23061405e-01\n",
      "   2.24025700e-02 -2.34844650e-02  1.44718747e-02 -3.07363055e-02\n",
      "   1.05461589e-01  2.38657807e-02  7.32938791e-02  6.86290325e-02\n",
      "   2.36960670e-02 -1.22849803e-01  1.20813838e-01  1.80589912e-01\n",
      "   1.32115883e-01  1.52642121e-01 -2.96125222e-02  4.53460875e-02\n",
      "   2.87003410e-02  1.09197691e-03 -4.90149763e-02  2.38547385e-03\n",
      "  -5.53127473e-02]\n",
      " [-1.22913215e-02  9.40907302e-02  8.65289481e-02  1.01633709e-01\n",
      "   3.32580064e-02  2.86044820e-03  4.07163679e-02 -1.97402812e-02\n",
      "   7.89577642e-02  2.57526355e-02  7.12079447e-02  4.86260191e-02\n",
      "   3.32674082e-02 -6.55058521e-02  8.65853593e-02  1.32115883e-01\n",
      "   1.01699522e-01  1.09364724e-01 -1.22161066e-02  4.84567855e-02\n",
      "   4.08197884e-02  3.03908368e-03 -1.23571346e-02  3.04848555e-03\n",
      "  -1.99189167e-02]\n",
      " [-2.43940456e-02  9.66432772e-02  1.02942809e-01  8.27353546e-02\n",
      "   1.00884840e-02 -2.55805213e-02 -3.80571997e-02  2.75558337e-02\n",
      "   1.05438144e-01  3.92131893e-02  3.56537623e-02  9.63985086e-02\n",
      "   1.38926796e-02 -9.44231962e-02  1.25767983e-01  1.52642121e-01\n",
      "   1.09364724e-01  1.44911345e-01  6.03951969e-03  2.79229866e-02\n",
      "   3.78895264e-03  4.66991963e-02 -5.10234153e-02  5.05033920e-02\n",
      "  -4.47238840e-02]\n",
      " [ 7.91028330e-02 -9.60582810e-03  1.03927044e-02 -3.65596622e-02\n",
      "   2.51870031e-02  4.95387204e-02 -3.30656882e-02  1.40833175e-01\n",
      "   2.69135861e-02  6.60514405e-02 -2.26408974e-02  7.56088589e-02\n",
      "   2.86646539e-02  1.14763037e-01  3.12586092e-02 -2.96125222e-02\n",
      "  -1.22161066e-02  6.03951969e-03  1.06924039e-01  1.30111445e-02\n",
      "   5.18847064e-03  1.15614086e-01  5.47592774e-02  1.19091736e-01\n",
      "   7.47578099e-02]\n",
      " [ 3.73263745e-02  5.07541618e-02  3.78545194e-02  6.97326117e-02\n",
      "   5.69755576e-02  5.40074480e-02  1.03308641e-01 -1.20461128e-02\n",
      "   2.79942808e-02  2.58394928e-02  7.58827133e-02  3.75034497e-03\n",
      "   5.39361539e-02  3.82109902e-02  1.96180969e-02  4.53460875e-02\n",
      "   4.84567855e-02  2.79229866e-02  1.30111445e-02  5.84596124e-02\n",
      "   6.98752000e-02 -3.74122316e-03  5.86022007e-02 -6.78062690e-03\n",
      "   4.57025584e-02]\n",
      " [ 4.70358232e-02  4.64651098e-02  2.59558359e-02  7.74362219e-02\n",
      "   7.07040046e-02  7.23616139e-02  1.48753388e-01 -3.62444796e-02\n",
      "   1.06774810e-02  1.88092163e-02  9.47865884e-02 -2.42813432e-02\n",
      "   6.54730856e-02  6.03984776e-02 -5.42967855e-03  2.87003410e-02\n",
      "   4.08197884e-02  3.78895264e-03  5.18847064e-03  6.98752000e-02\n",
      "   9.12132786e-02 -2.70258484e-02  8.36522567e-02 -3.22567675e-02\n",
      "   6.31429827e-02]\n",
      " [ 6.12794526e-02  1.29945812e-04  3.11799536e-02 -4.45037202e-02\n",
      "   4.02415936e-03  1.95549244e-02 -1.01735969e-01  1.73831410e-01\n",
      "   5.54381322e-02  7.58251419e-02 -4.93484426e-02  1.18511701e-01\n",
      "   1.08159885e-02  7.48746331e-02  7.19309284e-02  1.09197691e-03\n",
      "   3.03908368e-03  4.66991963e-02  1.15614086e-01 -3.74122316e-03\n",
      "  -2.70258484e-02  1.48599678e-01  1.37366486e-02  1.55391507e-01\n",
      "   4.47866564e-02]\n",
      " [ 9.10692100e-02 -9.23303405e-04 -1.21831801e-02  1.94140564e-02\n",
      "   7.23923800e-02  9.99727386e-02  1.33578414e-01  2.53444344e-02\n",
      "  -1.89043152e-02  3.39000539e-02  6.06106397e-02 -2.30951476e-02\n",
      "   6.78536384e-02  1.48412321e-01 -3.94156295e-02 -4.90149763e-02\n",
      "  -1.23571346e-02 -5.10234153e-02  5.47592774e-02  5.86022007e-02\n",
      "   8.36522567e-02  1.37366486e-02  1.22840401e-01  9.19790707e-03\n",
      "   1.11580524e-01]\n",
      " [ 6.04816941e-02 -4.48889167e-04  3.26850126e-02 -4.82353015e-02\n",
      "   8.77134234e-04  1.61926565e-02 -1.12845576e-01  1.81530659e-01\n",
      "   5.84926590e-02  7.79685677e-02 -5.48985450e-02  1.25091968e-01\n",
      "   8.20338952e-03  7.26313475e-02  7.66425443e-02  2.38547385e-03\n",
      "   3.04848555e-03  5.05033920e-02  1.19091736e-01 -6.78062690e-03\n",
      "  -3.22567675e-02  1.55391507e-01  9.19790707e-03  1.62717762e-01\n",
      "   4.23318088e-02]\n",
      " [ 9.43965909e-02 -9.84303522e-03 -1.20413919e-02 -2.73498328e-03\n",
      "   6.09446260e-02  9.14287614e-02  9.01463067e-02  6.27400627e-02\n",
      "  -1.17849010e-02  4.40171836e-02  3.51136950e-02  1.51530100e-03\n",
      "   5.84897784e-02  1.52653523e-01 -2.67704777e-02 -5.53127473e-02\n",
      "  -1.99189167e-02 -4.47238840e-02  7.47578099e-02  4.57025584e-02\n",
      "   6.31429827e-02  4.47866564e-02  1.11580524e-01  4.23318088e-02\n",
      "   1.09382168e-01]]\n"
     ]
    }
   ],
   "source": [
    "H = np.dot(np.dot(X, np.linalg.inv(np.dot(X.T,X))), X.T) \n",
    "print('H = ')\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  i       ei        hii         ti    \n",
      "  1    3.307787   0.085485   0.649988 \n",
      "  2    5.493973   0.088065   1.100843 \n",
      "  3   -2.524810   0.079287  -0.492376 \n",
      "  4   -1.156637   0.127755  -0.230709 \n",
      "  5   -0.454591   0.059256  -0.087217 \n",
      "  6    0.211475   0.083438   0.041099 \n",
      "  7   -2.075658   0.270959  -0.454510 \n",
      "  8   -4.484195   0.209026  -0.958378 \n",
      "  9   -0.691342   0.081989  -0.134305 \n",
      " 10   -6.655796   0.051904  -1.323779 \n",
      " 11   -2.808615   0.113523  -0.559120 \n",
      " 12    1.228612   0.122195   0.244326 \n",
      " 13   -2.216097   0.053497  -0.425629 \n",
      " 14   -9.348887   0.229696  -2.198011 \n",
      " 15    2.929787   0.116587   0.584653 \n",
      " 16   12.646853   0.180590   3.156277 \n",
      " 17    9.035552   0.101700   1.923641 \n",
      " 18    7.244067   0.144911   1.537353 \n",
      " 19   -4.459348   0.106924  -0.894510 \n",
      " 20   -5.423741   0.058460  -1.067822 \n",
      " 21    2.898127   0.091213   0.569978 \n",
      " 22   -4.701666   0.148600  -0.968994 \n",
      " 23   -0.202010   0.122840  -0.040132 \n",
      " 24    0.172044   0.162718   0.034983 \n",
      " 25    2.035116   0.109382   0.402766 \n"
     ]
    }
   ],
   "source": [
    "# y against x1 and x2 \n",
    "lm = smf.ols('JobProficiency~Test1+Test3', data=df).fit() \n",
    "X = df_1[['Test1', 'Test3']].to_numpy() \n",
    "X = sm.add_constant(X) \n",
    "y = df['JobProficiency'].to_numpy() \n",
    "# Find the H matrix \n",
    "H = np.dot(np.dot(X, np.linalg.inv(np.dot(X.T,X))), X.T)\n",
    "# Find standardized delted residuals (externally studentized residuals) \n",
    "t = lm.resid * np.sqrt((n-3-1)/(lm.ssr*(1-np.diag(H))-lm.resid**2)) \n",
    "print('{:^5} {:^10} {:^10} {:^10}'.format('i', 'ei', 'hii', 'ti')) \n",
    "for i in range(1, n+1):\n",
    "    print('{:^5d} {:^10f} {:^10f} {:^10f}'.format(i, lm.resid[i-1], H[i-1,i-1],\n",
    "t[i-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal elements of the hat matrix are shown above in the third column.\n",
    "\n",
    "Now, we will use the rule of thumb described in the notes to identify any outlying $x$ observations.\n",
    "\n",
    "From the notes we know that $h_{ii}$, the diagonal elements of the hat matrix, is the measure of the distance between the $x$ values for the ith case and the means of the $x$ values for all $n$ cases.\n",
    "\n",
    "Thus, a large $h_{ii}$ value indicates that the ith case is distant from the center of all $x$ observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal element $h_{ii}$ in this contect is called the leverage (in terms of the $x$ values) of the ith case.\n",
    "\n",
    "The figure below illustrates the role of the leverage values $h_{ii}$ as distance measures for Example 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#x1 va x3 Scatter Plot\n",
    "df_1.plot(x='Test1', y='Test3', kind='scatter', marker='o', s=50)\n",
    "for i in range(n): \n",
    "    plt.text(df.iloc[i,0], df.iloc[i,1], str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculatd the leverage values for $h_{ii}$ earlier. Recall that the largest leverage values are $h_{7,7} = 0.270959$, $h_{8,8} = 0.209026$, and $h_{14, 14} = 0.229696$. \n",
    "\n",
    "Note that the leverage value is \n",
    "\n",
    "$$\\frac{2p}{n} = \\frac{4}{25} = 0.16$$.\n",
    "\n",
    "Since the two largest leverage values both exceed the criterion of twice the mean leverage values and are separated by a substantial gap from the next largest leverage values which are $h_{16, 16} = 0.180590$ amd $h_{24, 24} = 0.162718$. \n",
    "\n",
    "Thus, cases 7, 8, and 14 are outlying $x$ observations as found by using the rule of tumb in the notes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C3) \n",
    "\n",
    "For those outlying cases in C1 & C2 obtain DFFITS, DFBETAS, & Cook's Distance values for these cases to asses their influence. \n",
    "\n",
    "What do you conclude??\n",
    "\n",
    "In (C1), the outlying $y$ observations were Case 19. \n",
    "\n",
    "In (C2), the outlying $x$ observations were the following:\n",
    "\n",
    "  1.) Case 7: $h_{7, 7} = 0.270959$\n",
    "  \n",
    "  2.) Case 8: $h_{8, 8} = 0.209026$\n",
    "   \n",
    "  3.) Case 14: $h_{14, 14} = 0.229696$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DFFITS value for the data is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dffits calculated from OLSInfluence is: \n",
      "88.0   86.0     0.077522\n",
      "80.0   62.0    -0.158041\n",
      "96.0   110.0   -0.033115\n",
      "76.0   101.0    0.498297\n",
      "80.0   100.0    0.001841\n",
      "73.0   78.0    -0.328906\n",
      "58.0   120.0   -0.396105\n",
      "116.0  105.0   -0.014482\n",
      "104.0  112.0    0.143260\n",
      "99.0   120.0   -0.352907\n",
      "64.0   87.0    -0.360572\n",
      "126.0  133.0    0.024800\n",
      "94.0   140.0    0.306935\n",
      "71.0   84.0     0.340976\n",
      "111.0  106.0   -0.318279\n",
      "109.0  109.0    0.748315\n",
      "100.0  104.0   -0.537806\n",
      "127.0  150.0    0.151498\n",
      "99.0   98.0     0.251871\n",
      "82.0   120.0   -0.111226\n",
      "67.0   74.0     0.543447\n",
      "109.0  96.0    -0.180128\n",
      "78.0   104.0   -0.715022\n",
      "115.0  94.0    -0.008212\n",
      "83.0   91.0     0.655510\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "lm = smf.ols('JobProficiency~Test1+Test3', data=df_1).fit() \n",
    "# Use OLSInfluence to find DFFITS:\n",
    "dffits = OLSInfluence(lm).dffits[0]\n",
    "print('dffits calculated from OLSInfluence is: ') \n",
    "print(dffits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, $(DFFITS)_7 = -0.3961$\n",
    "\n",
    "$(DFFITS)_8 = -0.014482$\n",
    "\n",
    "$(DFFITS)_{14} = 0.340976$\n",
    "\n",
    "$(DFFITS)_{19} = 0.251871$\n",
    "\n",
    "A guideline for identifying influential cases for DFFITS, the notes suggest considering a case influential if the absolute value of DFFITS exceeds 1 for small to medium data sets. Thus, none of the cases that we are considering would be considered influential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will obtain the DFBETAS values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFBETAS calculated from OLSInfluence is: \n",
      "[[ 9.15441883e-03  4.10092897e-02 -5.63355020e-02]\n",
      " [-3.16876680e-02  1.03429688e-01 -1.14646884e-01]\n",
      " [ 3.81905739e-03  1.39059591e-02 -2.25345720e-02]\n",
      " [ 2.75186021e-01 -4.12897988e-01  3.28218113e-01]\n",
      " [ 1.13566841e-03 -4.10278540e-04 -2.81087549e-04]\n",
      " [-1.70344869e-01 -4.74968770e-02  1.82053539e-01]\n",
      " [-3.76434531e-01  2.07864255e-01  2.49607122e-02]\n",
      " [ 1.05458243e-02 -1.22446205e-02  6.81288865e-03]\n",
      " [-5.07776130e-02 -2.76516475e-02  8.31509994e-02]\n",
      " [ 1.23211767e-01 -1.54409305e-01  7.79305044e-02]\n",
      " [-2.84328856e-01  2.55968677e-01 -1.14992643e-01]\n",
      " [-1.86421218e-02  9.34060985e-03  3.95580619e-03]\n",
      " [ 1.68827490e-01 -5.47537980e-02 -4.69991049e-02]\n",
      " [ 5.65131539e-02  2.11034420e-01 -3.06446141e-01]\n",
      " [ 1.58668515e-01  3.75437535e-02 -1.88420183e-01]\n",
      " [-1.52350092e-02 -4.84584332e-01  6.58460219e-01]\n",
      " [-6.18702000e-02  3.46071373e-01 -4.17740998e-01]\n",
      " [-4.92978836e-02 -5.50166261e-02  1.15677322e-01]\n",
      " [-1.14873242e-01  1.99209775e-01 -1.52852561e-01]\n",
      " [-6.92178570e-02  4.70317826e-02 -1.11265102e-02]\n",
      " [ 4.35731369e-01 -2.34325553e-01 -2.43059464e-02]\n",
      " [ 1.33916813e-01 -1.34858313e-01  5.91379133e-02]\n",
      " [-3.98285852e-01 -1.29971007e-01  4.58512041e-01]\n",
      " [ 6.30139541e-03 -6.13959377e-03  2.54062390e-03]\n",
      " [ 2.04653283e-01  2.76843734e-01 -4.92359498e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Use OLSInfluence to find DFBETAS:\n",
    "dfbetas = OLSInfluence(lm).dfbetas \n",
    "print('DFBETAS calculated from OLSInfluence is: ') \n",
    "print(dfbetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Case 7 the DFBETAS values are as follows: [-3.76434531e-01  2.07864255e-01  2.49607122e-02]\n",
    "\n",
    "For Case 8: [ 1.05458243e-02 -1.22446205e-02  6.81288865e-03]\n",
    "\n",
    "For Case 14: [ 5.65131539e-02  2.11034420e-01 -3.06446141e-01]\n",
    "\n",
    "For Case 19:  [-1.14873242e-01  1.99209775e-01 -1.52852561e-01]\n",
    "\n",
    "Following the guideline for identifying influential cases from the notes, that we should consider a case influential if the absolute value of DFBETAS exceeds 1 for small for medium data sets (recall that n=25).\n",
    "\n",
    "Notice that none of the absoulte values of DFBETAS exceeds 1 for any of the DFBETAS values we are considering. Hence, we would consider none of these cases influential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will obtain Cook's Distance values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cooks distance calculated from OLSInfluence is: \n",
      "88.0   86.0     0.002092\n",
      "80.0   62.0     0.008616\n",
      "96.0   110.0    0.000383\n",
      "76.0   101.0    0.080231\n",
      "80.0   100.0    0.000001\n",
      "73.0   78.0     0.035754\n",
      "58.0   120.0    0.053711\n",
      "116.0  105.0    0.000073\n",
      "104.0  112.0    0.007089\n",
      "99.0   120.0    0.039240\n",
      "64.0   87.0     0.043307\n",
      "126.0  133.0    0.000215\n",
      "94.0   140.0    0.030479\n",
      "71.0   84.0     0.039860\n",
      "111.0  106.0    0.034128\n",
      "109.0  109.0    0.174441\n",
      "100.0  104.0    0.090048\n",
      "127.0  150.0    0.007963\n",
      "99.0   98.0     0.021608\n",
      "82.0   120.0    0.004279\n",
      "67.0   74.0     0.090458\n",
      "109.0  96.0     0.011231\n",
      "78.0   104.0    0.152094\n",
      "115.0  94.0     0.000024\n",
      "83.0   91.0     0.128623\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Use OLSInfluence to find Cook's distance:\n",
    "D = OLSInfluence(lm).cooks_distance[0] \n",
    "print('Cooks distance calculated from OLSInfluence is: ')\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Case 7 the Cook's distance value is: $D_7 = 0.053711$\n",
    "\n",
    "For Case 8: $D_8 = 0.000073$\n",
    "\n",
    "For Case 14: $D_{14} =  0.030479$\n",
    "\n",
    "For Case 19: $D_{19} = 0.007963$\n",
    "\n",
    "We see that Case 7 has the largest $D_i$ value, with the next largest distance measure being $D_14 = 0.030479$, which is not that much smaller than $D_7$. However, the next largest distance measure is $D_19 = 0.007963$, which is much smaller than the other two distances. \n",
    "\n",
    " The ﬁgures below present the information provided by Cook’s distance measure about the inﬂuence of each case in two different plots. Shown in the left ﬁgure is a proportional inﬂuence plot of the residuals $e_i$ against the corresponding ﬁtted values $\\hat{y}_i$, the size of the plotted points being proportional to Cook’s distance measure $D_i$. The right ﬁgure resents the information about the Cook’s distance measures in the form of an index inﬂuence plot, where Cook’s distance measure $D_i$ is plotted against the corresponding case index $i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "sizevalues = np.abs(D)*500 \n",
    "plt.scatter(lm.fittedvalues, lm.resid, marker='o', s=sizevalues) \n",
    "plt.xlabel('$\\hat{y}$') \n",
    "plt.ylabel('Residual')\n",
    "plt.title('Proportional Influence Plot')\n",
    "\n",
    "plt.subplot(1,2,2) \n",
    "plt.plot(np.arange(1, n+1), D, 'o-') \n",
    "plt.xlabel('Case Index Number')\n",
    "plt.ylabel('Cooks Distance D') \n",
    "plt.title('Index Influence Plot')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the graphs we can see that the cases we are considering are considered less influential & cases that we are not considering are very influential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (C4)\n",
    "\n",
    "Obtain the variance inflation factors. \n",
    "\n",
    "What do they indicate??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will obtain the variance inflation factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Variable     (VIF)k   \n",
      "    x1       1.033781  \n",
      "    x2       1.033781  \n",
      "max vif =  1.033781399684168\n",
      "mean_vif =  1.033781399684168\n"
     ]
    }
   ],
   "source": [
    "# Get X variables \n",
    "X = df[['Test1', 'Test3']].to_numpy()\n",
    "\n",
    "# Run vif for each predictor:\n",
    "nparam = 2 \n",
    "vif = np.zeros(nparam,)\n",
    "\n",
    "for i in range(nparam): \n",
    "    vif[i] = variance_inflation_factor(zscore(X), [i])\n",
    "    \n",
    "# print out the results: \n",
    "print('{:^10} {:^12}'.format('Variable', '(VIF)k')) \n",
    "for i in range(nparam): \n",
    "    print('{:^10} {:^12f}'.format('x'+str(i+1), vif[i]))\n",
    "    \n",
    "# Find the maximu vif and the mean: \n",
    "max_vif = vif.max() \n",
    "mean_vif = vif.mean()\n",
    "\n",
    "print('max vif = ', max_vif) \n",
    "print('mean_vif = ', mean_vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be see that multicollinearity among the two predictor variables is not a problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "name": "Lecture1.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
